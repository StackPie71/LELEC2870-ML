{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Session 4: Model & Feature Selection\n",
    "\n",
    "## 4.1 Objectives\n",
    "As you may have noticed during the previous sessions, two practices are commonly used when solving a machine learning problem. The first one, introduced in Session 1, is feature selection. Indeed, some models are very sensitive to the presence of irrelevant features which affect, for instance, distance measures. The second practice, introduced in Session 2, is model selection. Indeed, most prediction models have one or several **meta-parameters** whose values have to be carefully chosen in order to get good model performances. Both the feature selection and the model selection procedures will be investigated more in depth in this session."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.io\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "import itertools\n",
    "\n",
    "from sklearn.feature_selection import mutual_info_regression\n",
    "\n",
    "from rbfn import MyRBFN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.2 Creating a dataset\n",
    "For this session, you have to build an artificial dataset containing 1000 observations. For each observation $x_i=\\left(x_{i,1},\\dots,x_{i,6}\\right)$, the values of the 6 features are randomly chosen in the interval $\\left[  0,1 \\right]$ (you can use `numpy.random.random_sample` for this). The target is then computed as:\n",
    "\\begin{equation}\n",
    "\tf(x_i) = 2 \\sin\\left(2 * x_{i,1}\\right) x_{i,2} + 4 \\left(x_{i,3}-.5\\right)^2 + x_{i,4} + \\epsilon_i\n",
    "\\end{equation}\n",
    "where $\\epsilon_i$ is a noise component following a normal distribution ${\\mathcal N}(0,0.01)$ (you can use `numpy.random.normal` for this). Before going ahead, take a look at the equation above. What can you say about the six features of our problem? Are they all equally useful?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_samples(nb_samples):\n",
    "    # TODO: generate x and f(x) according to the equation above\n",
    "    # x.shape = (nb_samples, 6)\n",
    "    # y.shape = (nb_samples, 1)\n",
    "    x = np.random.random_sample((nb_samples,6))\n",
    "    noise = np.random.normal(0,0.01,(nb_samples,))\n",
    "    y = 2*np.sin(2*x[:,0])*x[:,1]+4*(x[:,2]-0.5)**2+x[:,3]+noise\n",
    "    return x,y[:,np.newaxis]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, y_train = draw_samples(1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.2 Feature selection\n",
    "Feature selection can be performed with a criterion that quantifies the pertinence of features for predicting the target. We will investigate the two following criteria: the correlation coefficient (using `numpy.corrcoef`) and the mutual information (using `sklearn.feature_selection.mutual_info_regression`). A simple feature selection strategy consists in selecting the features achieving a sufficiently high score for a given criterion. Implement this strategy and apply it on your dataset.\n",
    "\n",
    "**Analyse** the results. Did you expect these results ? Are they coherent with the equation above? Are both criteria appropriate for our task? \n",
    "\n",
    "Before moving to the next step, **build a reduced training** set only containing the features you have selected. Keep also a copy of the complete training set in order to make comparisons and to assess the interest of feature selection.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Correlation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.40829845,  0.60297706, -0.00147822,  0.40036916,  0.02219411,\n",
       "       -0.05806866])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TODO: compute the Correlation between the output (y) and the features (x_i)\n",
    "np.corrcoef(x_train,y=y_train,rowvar= False)[-1,:-1]\n",
    "#corr between -1 and 1\n",
    "#for values 5 and 6 no correlation OK BUT for value 3 no corr is not OK"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mutual information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.10833977, 0.25583933, 0.12214406, 0.12547215, 0.0165314 ,\n",
       "       0.01825161])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TODO: compute the Mutual Information between the output (y) and the features (x_i)\n",
    "mutual_info_regression(x_train,y_train[:,0])\n",
    "#we have no MI 5 and 6 OK and MI for 3 is non null so ok\n",
    "#MI is from 0 to +inf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.3 Model selection\n",
    "RBFNs that you have implemented in session 3 have two meta-parameters (the number of centers and the smoothing factor). These meta-parameters cannot be optimized directly using a training set since their role is to control the model complexity and to prevent under/overfitting. In the remaining of this session, you will **implement a simple validation procedure** which allows to select good values of the meta-parameters for a specific training set. To train RBFNs, you can either use your own code or the one we provide on Moodle (`rbfn.py`).\n",
    "**Divide the dataset** you just created in a **training** set ($70\\%$) and a **validation** set ($30\\%$). Then, build a grid (of reasonable size) of the values you will test for the meta-parameters. For each pair of values (number of centers, smoothing factor), train a RBFN using the training set and measure the error made on the validation set. Use the results to select the appropriate number of centers and the smoothing factor for your problem. Eventually, train a RBFN on the whole dataset using the chosen meta-parameters.\n",
    "\n",
    "**Build a test set** containing 10000 samples. Measure the error made on these data using your model. Repeat the whole procedure detailed in this section using all the features. How do the results compare to the case where feature have been selected? Is this always the case?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: split in train and test\n",
    "x_train_reduced = x_train[:,(0,1,2,3)]\n",
    "\n",
    "x_test,y_test = draw_samples(10000)\n",
    "x_test_reduced = x_test[:,(0,1,2,3)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: apply meta-parameter search for RBFN on the training set\n",
    "def model_selection(x_train,y_train,x_val,y_val):\n",
    "    best_score=np.inf\n",
    "    best_parameters = (0,0)\n",
    "    for nb_centers in (10,25,50,75,100,150):\n",
    "        for width_scaling in (0.1,0.2,0.5,1.,5.,10.,20.):\n",
    "            rbfn = MyRBFN(nb_centers,width_scaling)\n",
    "            rbfn.fit(x_train,y_train)\n",
    "            score = rbfn.score(x_val,y_val)\n",
    "            print(score)\n",
    "            if score < best_score:\n",
    "                best_score = score\n",
    "                best_parameters = (nb_centers,width_scaling)\n",
    "    return best_parameters, best_score\n",
    "\n",
    "def evaluate(x_train, y_train, best_parameters, x_test, y_test):\n",
    "    rbfn = MyRBFN(best_parameters[0],best_parameters[1])\n",
    "    rbfn.fit(x_train,y_train)\n",
    "    score = rbfn.score(x_test,y_test)\n",
    "    return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "216.52046944327023\n",
      "0.6959565769821802\n",
      "0.5906822005729948\n",
      "0.4259114865601797\n",
      "0.37151829164455175\n",
      "0.3593842455815681\n",
      "0.3733597963067306\n",
      "Stopped at epoch 63\n",
      "912383.6367337966\n",
      "Stopped at epoch 69\n",
      "1.8418115337969712\n",
      "Stopped at epoch 61\n",
      "0.5584799248204951\n",
      "Stopped at epoch 61\n",
      "0.3941926478114398\n",
      "Stopped at epoch 62\n",
      "0.17664338533578453\n",
      "Stopped at epoch 62\n",
      "0.18153316217721552\n",
      "Stopped at epoch 58\n",
      "0.2227260450783657\n",
      "Stopped at epoch 35\n",
      "52268.23964340689\n",
      "Stopped at epoch 33\n",
      "1.6900045586698154\n",
      "Stopped at epoch 32\n",
      "0.5520080771869882\n",
      "Stopped at epoch 33\n",
      "0.3502765803984098\n",
      "Stopped at epoch 34\n",
      "0.06184699178797489\n",
      "Stopped at epoch 36\n",
      "0.06310700994333103\n",
      "Stopped at epoch 31\n",
      "1.3984813551539712\n",
      "Stopped at epoch 23\n",
      "72974094.87021206\n",
      "Stopped at epoch 23\n",
      "10.03009677174635\n",
      "Stopped at epoch 24\n",
      "0.5547088478323865\n",
      "Stopped at epoch 23\n",
      "0.2919496101906804\n",
      "Stopped at epoch 24\n",
      "0.04591968515118244\n",
      "Stopped at epoch 21\n",
      "0.04520538867240998\n",
      "Stopped at epoch 23\n",
      "52.09052983509665\n",
      "Stopped at epoch 17\n",
      "36329.846446564545\n",
      "Stopped at epoch 18\n",
      "29.288938303177968\n",
      "Stopped at epoch 17\n",
      "0.5888328647242709\n",
      "Stopped at epoch 18\n",
      "0.29125859958934847\n",
      "Stopped at epoch 18\n",
      "0.04115887756404427\n",
      "Stopped at epoch 19\n",
      "0.019171334316785617\n",
      "Stopped at epoch 18\n",
      "4.117556485774641\n",
      "Stopped at epoch 13\n",
      "60354180240.88454\n",
      "Stopped at epoch 13\n",
      "4.523860105709048\n",
      "Stopped at epoch 13\n",
      "0.6193418833095103\n",
      "Stopped at epoch 16\n",
      "0.3204178556512787\n",
      "Stopped at epoch 13\n",
      "0.017167764790021373\n",
      "Stopped at epoch 13\n",
      "1.2775850188939577\n",
      "Stopped at epoch 14\n",
      "5.944973484260637\n",
      "(150, 5.0) 0.017167764790021373\n"
     ]
    }
   ],
   "source": [
    "# TODO: Do the same but now apply feature selection\n",
    "best_parameters, best_score = model_selection(x_train[:700,:],y_train[:700],x_train[700:,:],y_val=y_train[700:])\n",
    "print(best_parameters, best_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stopped at epoch 18\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.015328965734096601"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluate(x_train,y_train,best_parameters,x_test,y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### With feature selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "729.8473566923778\n",
      "0.6898799557955133\n",
      "0.5599581866637927\n",
      "0.40531627839828427\n",
      "0.33221450878427783\n",
      "0.31356940270133926\n",
      "0.3257529093356801\n",
      "Stopped at epoch 52\n",
      "2386584.0500843087\n",
      "Stopped at epoch 54\n",
      "1.4580103871790726\n",
      "Stopped at epoch 54\n",
      "0.5430594567913277\n",
      "Stopped at epoch 51\n",
      "0.3265759307280358\n",
      "Stopped at epoch 51\n",
      "0.06876795839020501\n",
      "Stopped at epoch 52\n",
      "0.053356367222977916\n",
      "Stopped at epoch 53\n",
      "0.3298794111794873\n",
      "Stopped at epoch 29\n",
      "1054.257685815239\n",
      "Stopped at epoch 29\n",
      "1.234035732011859\n",
      "Stopped at epoch 28\n",
      "0.5505084138902425\n",
      "Stopped at epoch 28\n",
      "0.30243974784038635\n",
      "Stopped at epoch 27\n",
      "0.019357868146828845\n",
      "Stopped at epoch 28\n",
      "0.041425542091269654\n",
      "Stopped at epoch 29\n",
      "3.4716265799863266\n",
      "Stopped at epoch 19\n",
      "35700906.125846855\n",
      "Stopped at epoch 18\n",
      "1.020054981149734\n",
      "Stopped at epoch 19\n",
      "0.5854118268332432\n",
      "Stopped at epoch 17\n",
      "0.2877075067141525\n",
      "Stopped at epoch 21\n",
      "0.013299094982684948\n",
      "Stopped at epoch 18\n",
      "0.23283061169396113\n",
      "Stopped at epoch 16\n",
      "1.1498257610943632\n",
      "Stopped at epoch 15\n",
      "72072.10182132508\n",
      "Stopped at epoch 14\n",
      "4.173514817348284\n",
      "Stopped at epoch 12\n",
      "0.5605029260171384\n",
      "Stopped at epoch 14\n",
      "0.2913312996355658\n",
      "Stopped at epoch 14\n",
      "0.012205099665671959\n",
      "Stopped at epoch 14\n",
      "3.7653819758499223\n",
      "Stopped at epoch 14\n",
      "81.0786729611506\n",
      "Stopped at epoch 11\n",
      "74017087246.94547\n",
      "Stopped at epoch 11\n",
      "68.31649630340819\n",
      "Stopped at epoch 11\n",
      "0.5881456901606412\n",
      "Stopped at epoch 10\n",
      "0.3124593345146381\n",
      "Stopped at epoch 10\n",
      "0.011365647216594148\n",
      "Stopped at epoch 11\n",
      "1.9353273676365887\n",
      "Stopped at epoch 10\n",
      "17.916121853860865\n",
      "(150, 5.0) 0.011365647216594148\n"
     ]
    }
   ],
   "source": [
    "best_parameters, best_score = model_selection(x_train_reduced[:700,:],y_train[:700],x_train_reduced[700:,:],y_val=y_train[700:])\n",
    "print(best_parameters, best_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stopped at epoch 13\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.011077832567678345"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluate(x_train_reduced,y_train,best_parameters,x_test_reduced,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
